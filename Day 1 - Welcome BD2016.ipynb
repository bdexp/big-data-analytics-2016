{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Big Data with Hadoop and Spark 2016!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Jupyter (aka iPython)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code is written in cells that are either markdown or code. A cell is executed with SHIFT-ENTER or CTRL-ENTER. This particular notebook is created with a PySpark kernel and python 2.7, which means that all python 2.7 commands are valid as well as interaction with the SparkContext object *sc* (Spark version 1.6.1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Python version\n",
    "import sys\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Spark version\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though you can use the buttons $\\uparrow$, it us a better workflow to use:\n",
    "\n",
    "SHIFT-ENTER - Execute cell and select cell below\n",
    "\n",
    "CTRL-ENTER - Execute cell and select same cell\n",
    "\n",
    "ESC - exit edit mode\n",
    "\n",
    "ENTER - enter edit mode\n",
    "\n",
    "a - insert cell above\n",
    "\n",
    "b - insert cell below\n",
    "\n",
    "j - select cell below\n",
    "\n",
    "k - select cell above\n",
    "\n",
    "m - change cell to markup\n",
    "\n",
    "y - change cell to code\n",
    "\n",
    "TAB - autocompletion suggestions\n",
    "\n",
    "\\### - for level of headline in markup. # is large, ## smaller, ### even smaller...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shell script using *!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!whoami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LaTeX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even use LaTeX in markdown: $\\lambda = \\frac{1}{\\omega}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "We will use matplotlib for plotting, see http://matplotlib.org/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [1, 2, 3, 4, 5]\n",
    "y = [2, 3, 5, 6, 4]\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Example Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark (pyspark)\n",
    "Spark is a framework for distributed computing. It runs in the JVM (Java Virtual Machine), but there are drivers for other languages as well, including Python. In python you interact with the SparkContext, sc, object to access the spark cluster. The api for python, scala and java are similar, so transition between the languages should not be a big problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.parallelize([1,2,3,4,5,6,7,8,9,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter data\n",
    "data.filter(lambda x: x<5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Map data\n",
    "data.map(lambda x: 2 * x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reduce\n",
    "data.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Composing functions\n",
    "data.map(lambda x: 2 * x).reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOW YOU TRY SOMETHING!!!\n",
    "# start with data. and press TAB\n",
    "# gettins started tutorial here: http://spark.apache.org/docs/latest/quick-start.html\n",
    "data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
