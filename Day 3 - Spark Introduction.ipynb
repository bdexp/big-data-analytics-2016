{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3 - Spark Introduction\n",
    "\n",
    "This notebook consists of basic exercises for start using the Spark DataFrame API. In the end you will also try out Spark SQL and GraphFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "The dataset that will be used is rating data from the [MovieLens](https://movielens.org/) website. Find information about the data [here](https://s3-eu-west-1.amazonaws.com/orvarsbucket/ml-latest/README.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data From S3\n",
    "\n",
    "[S3](http://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html) is a scalable storage service provided by AWS. Two datasets is uploaded to a bucket and can be fetched by the provided access keys. The smaller one can be used for testing purposes.\n",
    "\n",
    "An external library called [spark-csv](https://github.com/databricks/spark-csv) can be used to parse csv-files for Spark.\n",
    "\n",
    "An example is given below with how to load an parse a CSV-file and load it into a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set access keys for the S3 bucket.\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", accessKeyId)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", secretAccessKey)\n",
    "\n",
    "# Paths. Change PATH_DATASET to ml-latest/ to get the larger dataset.\n",
    "PATH_BUCKET = 's3n://orvarsbucket/'\n",
    "PATH_DATASET = 'ml-latest-small/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a Spark SQL context which is required to work with the DataFrame API.\n",
    "sql_context = SQLContext(sc)\n",
    "\n",
    "# Read ratings.csv\n",
    "filename = 'ratings.csv'\n",
    "ratings_schema = StructType([ \\\n",
    "    StructField(\"userId\", StringType(), True), \\\n",
    "    StructField(\"movieId\", StringType(), True), \\\n",
    "    StructField(\"rating\", FloatType(), True), \\\n",
    "    StructField(\"timestamp\", IntegerType(), True), \\\n",
    "])\n",
    "\n",
    "ratings_df = sql_context.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true') \\\n",
    "    .load(PATH_BUCKET + PATH_DATASET + filename, schema=ratings_schema)\n",
    "    \n",
    "print 'Loaded ' + str(ratings_df.count()) + ' entries from ' + filename + '\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**EXERCISE: **</font>Load and parse the remaining datasets containing data about links, movies and tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'links.csv'\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The DataFrame API\n",
    "\n",
    "In Spark, a DataFrame is a distributed collection of data organized into named columns. The DataFrame API can be used to perform various relational operations on these collections. Read more about Spark SQL and the DataFrame API [here](http://spark.apache.org/docs/latest/sql-programming-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations\n",
    "\n",
    "The DataFrames offers various different high-level operations for processing your dataset, as displayed with the  **count**-operation in previous code. For example you have **take**, **show** and **collect** to display content from a DataFrame. All these operation are called actions and are executed immediately (eager), unlike transformation (e.g. map, filter etc.) which are performed lazy.\n",
    "\n",
    "<font color='red'>**EXERCISE: **</font>Try out the various methods for displaying DataFrame content. Figure out why you should think twice before using the **collect** operation.\n",
    "\n",
    "*Tips: Use TAB for code completion.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratings_df.take(2)\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching\n",
    "\n",
    "A very useful feature in Spark is the ability to avoid reading data from disk by caching it into memory. This is performed by using the **cache** operation.\n",
    "\n",
    "<font color='red'>**EXERCISE: **</font> Change the PATH_DATASET variable and read in the larger dataset. Enable timing for your cell execution by looking at the example below. Compare the result of an action, e.g. **count**, using a dataset read from disk with a dataset cached into memory.\n",
    "\n",
    "*Tips: Remember that the **cache** operation is performed lazy.*\n",
    "\n",
    "*Tips 2: Data can be un-cached with the **unpersist** operation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "\n",
    "# Sleep for 5 seconds.\n",
    "time.sleep(5)\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Filters and Aggregations\n",
    "\n",
    "For a  visual guide for filters, aggregations and other types of transformations, check [this](http://nbviewer.jupyter.org/github/jkthompson/pyspark-pictures/blob/master/pyspark-pictures.ipynb) link.\n",
    "\n",
    "<font color='red'>**EXERCISE: **</font>Count the number of movies related to the Action genre.\n",
    "\n",
    "*Tips: Remember that the genres column can consist of multiple genres!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**EXERCISE: **</font>Group the rating data by year and count the number of ratings each year.\n",
    "\n",
    "*Tips: Use the **withColumn** operation to create a new column in the DataFrame.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining DataFrames\n",
    "\n",
    "To process data over multiple DataFrames one needs to join them together and the **join** operation can help with that.\n",
    "\n",
    "<font color='red'>**EXERCISE: **</font>Print the link to the IMDB page of your favourite movie.\n",
    "\n",
    "*Tips: Link to IMDB movies has the syntax http://www.imdb.com/title/tt + imdbId.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### SQL Queries\n",
    "\n",
    "Data in Spark can also be inserted into tables and queried with a SQL-like syntax through Spark's SQLContext. When registering a table, one can infer a schema through reflecting an existing DataFrame or programmatically specify it as we did when we read the data the first time. Since the data already exists in DataFrames we can infer the schemas from those.\n",
    "\n",
    "<font color='red'>**EXERCISE: **</font>Use SQL query syntax and count how many movies that have received the highest rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# registerTempTable creates an in-memory table scoped to the cluster. \n",
    "# saveAsTable is another function which instead creates a physical table stored in S3, and can thus be used in other clusters.\n",
    "ratings_df.registerTempTable(\"ratings\")\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphFrames\n",
    "\n",
    "[GraphX](http://spark.apache.org/docs/latest/graphx-programming-guide.html) is a new component in Spark used for graphs and graph-parallel computation. GraphX is to RDDs as [GraphFrames](http://graphframes.github.io/index.html) are to DataFrames, so GraphFrames is just another interface to work with graphs in similar way that you have done with the DataFrames.\n",
    "\n",
    "<font color='red'>**EXERCISE: **</font>Below is the start of a simple example from the homepage. Use the GraphFrames API and do the section about [Motif finding](http://graphframes.github.io/user-guide.html#motif-finding) to get a feeling of how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "\n",
    "# Create a Vertex DataFrame with unique ID column \"id\"\n",
    "verts = sql_context.createDataFrame([\n",
    "    (\"a\", \"Alice\", 34),\n",
    "    (\"b\", \"Bob\", 36),\n",
    "    (\"c\", \"Charlie\", 30),\n",
    "    (\"d\", \"David\", 29),\n",
    "    (\"e\", \"Esther\", 32),\n",
    "    (\"f\", \"Fanny\", 36)\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "# Create an Edge DataFrame with \"src\" and \"dst\" columns\n",
    "edges = sql_context.createDataFrame([\n",
    "    (\"a\", \"b\", \"friend\"),\n",
    "    (\"b\", \"c\", \"follow\"),\n",
    "    (\"c\", \"b\", \"follow\"),\n",
    "    (\"f\", \"c\", \"follow\"),\n",
    "    (\"e\", \"f\", \"follow\"),\n",
    "    (\"e\", \"d\", \"follow\"),\n",
    "    (\"d\", \"a\", \"follow\")\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "\n",
    "# Create a GraphFrame\n",
    "graph = GraphFrame(verts, edges)\n",
    "\n",
    "# Search for pairs of vertices with edges in both directions between them.\n",
    "motifs = graph.find(\"(a)-[e1]->(b); (b)-[e2]->(a)\")\n",
    "motifs.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
