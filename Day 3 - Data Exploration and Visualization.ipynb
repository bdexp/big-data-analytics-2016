{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAY 3 - Data Exploration and Visualization\n",
    "\n",
    "Exploratory data analysis and visualization is an important step in the data analytics pipeline. Cleaning and exploring the dataset is often an iterative process before developing the actual models and algorithms that are used to give knowledge and insight. This notebook lets you combined the knowledge you have recieved from using Spark with producing more graphical content to display information in a way such that becomes more perceivable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "The dataset is rating data from the [MovieLens](https://movielens.org/) website. Find information about the data [here](https://s3-eu-west-1.amazonaws.com/orvarsbucket/ml-latest/README.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data From S3\n",
    "\n",
    "[S3](http://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html) is a scalable storage service provided by AWS. Two datasets is uploaded to a bucket and can be fetched by the provided access keys. The smaller one can be used for testing purposes.\n",
    "\n",
    "An external library called [spark-csv](https://github.com/databricks/spark-csv) can be used to parse csv-files for Spark.\n",
    "\n",
    "**Exercise:** Create Dataframes for the datasets and perform simple counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set access keys for S3 bucket.\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", accessKeyId)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", secretAccessKey)\n",
    "\n",
    "# Paths. Change PATH_DATASET to ml-latest/ to get the larger dataset.\n",
    "PATH_BUCKET = 's3n://orvarsbucket/'\n",
    "PATH_DATASET = 'ml-latest-small/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create Spark SQL context.\n",
    "sql_context = SQLContext(sc)\n",
    "\n",
    "# Read links.csv\n",
    "filename = 'links.csv'\n",
    "links_schema = StructType([ \\\n",
    "    StructField(\"movieId\", StringType(), True), \\\n",
    "    StructField(\"imdbId\", StringType(), True), \\\n",
    "    StructField(\"tmdbId\", StringType(), True), \\\n",
    "])\n",
    "\n",
    "links_df = sql_context.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true') \\\n",
    "    .load(PATH_BUCKET + PATH_DATASET + filename, schema=links_schema)\n",
    "\n",
    "links_df.cache()\n",
    "print 'Loaded ' + str(links_df.count()) + ' entries from ' + filename + '\\n'\n",
    "    \n",
    "# Read movies.csv\n",
    "filename = 'movies.csv'\n",
    "movies_schema = StructType([ \\\n",
    "    StructField(\"movieId\", StringType(), True), \\\n",
    "    StructField(\"title\", StringType(), True), \\\n",
    "    StructField(\"genres\", StringType(), True), \\\n",
    "])\n",
    "\n",
    "movies_df = sql_context.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true') \\\n",
    "    .load(PATH_BUCKET + PATH_DATASET + filename, schema=movies_schema)\n",
    "\n",
    "movies_df.cache()\n",
    "print 'Loaded ' + str(movies_df.count()) + ' entries from ' + filename + '\\n'\n",
    "    \n",
    "# Read ratings.csv\n",
    "filename = 'ratings.csv'\n",
    "ratings_schema = StructType([ \\\n",
    "    StructField(\"userId\", StringType(), True), \\\n",
    "    StructField(\"movieId\", StringType(), True), \\\n",
    "    StructField(\"rating\", FloatType(), True), \\\n",
    "    StructField(\"timestamp\", IntegerType(), True), \\\n",
    "])\n",
    "\n",
    "ratings_df = sql_context.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true') \\\n",
    "    .load(PATH_BUCKET + PATH_DATASET + filename, schema=ratings_schema)\n",
    "\n",
    "ratings_df.cache()\n",
    "print 'Loaded ' + str(ratings_df.count()) + ' entries from ' + filename + '\\n'\n",
    "    \n",
    "# Read tags.csv\n",
    "filename = 'tags.csv'\n",
    "tags_schema = StructType([ \\\n",
    "    StructField(\"userId\", StringType(), True), \\\n",
    "    StructField(\"movieId\", StringType(), True), \\\n",
    "    StructField(\"tag\", StringType(), True), \\\n",
    "    StructField(\"timestamp\", IntegerType(), True), \\\n",
    "])\n",
    "\n",
    "tags_df = sql_context.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true') \\\n",
    "    .load(PATH_BUCKET + PATH_DATASET + filename, schema=tags_schema)\n",
    "\n",
    "tags_df.cache()\n",
    "print 'Loaded ' + str(tags_df.count()) + ' entries from ' + filename + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Matplotlib\n",
    "\n",
    "[Matplotlib](http://matplotlib.org/gallery.html) is an excellent graphics library in Python for generating simple visualizations. Follow the simple example below to understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aggregate data by using Spark\n",
    "ratings_gby_df = ratings_df.withColumn(\"year\", year(from_unixtime(ratings_df[\"timestamp\"]))) \\\n",
    "    .groupBy(\"year\") \\\n",
    "    .count()\n",
    "    \n",
    "data_x = ratings_gby_df.select(\"year\").flatMap(lambda x: x).collect()\n",
    "data_y = ratings_gby_df.select(\"count\").flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This line configures matplotlib to show figures embedded in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "# Import matplotlib, the easy way\n",
    "from pylab import *\n",
    "\n",
    "# Create a new matplotlib figure with a specific size.\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "\n",
    "# Add subplot in figure and retrieve axes. \"111\" means \"1x1 grid, first subplot\"\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Remove plot frame lines\n",
    "ax.spines[\"top\"].set_visible(False)       \n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "# Only add ticks at bottom/left\n",
    "ax.get_xaxis().tick_bottom()\n",
    "ax.get_yaxis().tick_left()\n",
    "\n",
    "# Edit ticks\n",
    "plt.xticks(data_x, fontsize=14, rotation='vertical')\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "# Insert labels\n",
    "plt.xlabel(\"Year\", fontsize=16)  \n",
    "plt.ylabel(\"Count\", fontsize=16)\n",
    "\n",
    "# Insert title\n",
    "plt.title(\"Number of ratings per year\", fontsize=18)\n",
    "\n",
    "# Create barchart\n",
    "ax.bar(\n",
    "    data_x,\n",
    "    data_y,\n",
    "    align=\"center\",\n",
    "    color=\"#3F5D7D\",\n",
    "    width=1.0,\n",
    "    alpha=1.0\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**EXERCISE: **</font>Explore the dataset through visualizations. Select a couple of aggregations that could be interesting to show and display them in a suitable format.\n",
    "\n",
    "Examples of visualizations that can be performed:\n",
    "\n",
    "* Compare the average ratings for each genre over time.\n",
    "* Show in which months, days of the week or/and hours that users are most active.\n",
    "\n",
    "Think of what type of visualization that is suitable for your subset of data. Read for example [this](http://blog.hubspot.com/marketing/data-visualization-choosing-chart#sm.00008fchn9p1dfecu9x2chwb2h5qi) and [this](http://img.labnol.org/di/data-chart-type.png).\n",
    "\n",
    "Use [Colorbrewer](http://colorbrewer2.org/) when choosing color palette.\n",
    "\n",
    "Check out [D3.js](https://github.com/d3/d3/wiki/Gallery) for inspiration.\n",
    "\n",
    "*Tips: Remember the [data-to-ink-ratio](http://static1.squarespace.com/static/56713bf4dc5cb41142f28d1f/t/5671eae2816924fc2265189a/1454121618204/data-ink.gif?format=750w) when producing visualizations!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Interatice Visualizations\n",
    "\n",
    "Sometimes when visualizing data, one wants more interactivity built into the visuals and not just static images. This is because the interaction gives another dimension of perception. There are other libaries in Python that targets web browsers for producing more interactive plots\n",
    "\n",
    "There are other libraries in Python that targets web browsers for producing more interactive plots. They are also often very high-level which makes them ideal for rapidly developing prototypes. One such tool is [Bokeh](http://bokeh.pydata.org/).\n",
    "\n",
    "<font color='red'>**EXERCISE: **</font>Use an interactive visualization tool, e.g. Bokeh, and find out if movies with higher ratings also has more ratings. Do this by producing a scatter plot (see example below) with for example number of ratings at one axis and average rating for the other axis. To make it more interactive one can add functionality such as displaying title in tooltip, color by genre etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aggregate data by using Spark\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook, show\n",
    "from bokeh import *\n",
    "\n",
    "# Load bokeh\n",
    "output_notebook(resources=resources.INLINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.sampledata.iris import flowers\n",
    "flowers.head()\n",
    "\n",
    "# Define data source\n",
    "df = flowers\n",
    "\n",
    "# Create figure with size and tools to use\n",
    "fig = figure(\n",
    "    plot_width=800,\n",
    "    plot_height=600,\n",
    "    tools=[\"wheel_zoom\", \"reset\"]\n",
    ")\n",
    "\n",
    "# Define colors\n",
    "def spec_color(species):\n",
    "    if species == \"virginica\":\n",
    "        return \"#de2d26\"\n",
    "    elif species == \"versicolor\":\n",
    "        return \"#2b8cbe\"\n",
    "    else:\n",
    "        return \"#31a354\"\n",
    "\n",
    "colors = [spec_color(s) for s in df[\"species\"]]\n",
    "\n",
    "# Define plot and data to use for axes\n",
    "fig.scatter(\n",
    "    df[\"petal_length\"],\n",
    "    df[\"petal_width\"], \n",
    "    source=ColumnDataSource(data=df),\n",
    "    size=10,\n",
    "    alpha=0.5,\n",
    "    color=colors\n",
    ")\n",
    "\n",
    "# Define axis labels\n",
    "fig.xaxis.axis_label = \"Petal Length\"\n",
    "fig.yaxis.axis_label = \"Petal Width\"\n",
    "\n",
    "# Create tooltips for markes to display extra information\n",
    "hover = HoverTool()\n",
    "hover.tooltips = [\n",
    "    (\"Sepal Length\", \"@sepal_length\"),\n",
    "    (\"Sepal Width\", \"@sepal_width\")\n",
    "]\n",
    "\n",
    "# Add tools to figure\n",
    "fig.add_tools(hover)\n",
    "\n",
    "# Show figure\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
